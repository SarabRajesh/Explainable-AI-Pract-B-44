{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SarabRajesh/Explainable-AI-Pract-B-44/blob/main/Weather_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bp96vMm3CPZ3",
        "outputId": "ca7ad691-ed3f-438e-bae4-ea465d68de66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ File loaded successfully!\n",
            "\n",
            "üìä First 5 rows of the dataset:\n",
            "         Date Location  MinTemp  MaxTemp  Rainfall  Evaporation  Sunshine  \\\n",
            "0  2008-12-01   Albury     13.4     22.9       0.6          NaN       NaN   \n",
            "1  2008-12-02   Albury      7.4     25.1       0.0          NaN       NaN   \n",
            "2  2008-12-03   Albury     12.9     25.7       0.0          NaN       NaN   \n",
            "3  2008-12-04   Albury      9.2     28.0       0.0          NaN       NaN   \n",
            "4  2008-12-05   Albury     17.5     32.3       1.0          NaN       NaN   \n",
            "\n",
            "  WindGustDir  WindGustSpeed WindDir9am  ... Humidity9am  Humidity3pm  \\\n",
            "0           W           44.0          W  ...        71.0         22.0   \n",
            "1         WNW           44.0        NNW  ...        44.0         25.0   \n",
            "2         WSW           46.0          W  ...        38.0         30.0   \n",
            "3          NE           24.0         SE  ...        45.0         16.0   \n",
            "4           W           41.0        ENE  ...        82.0         33.0   \n",
            "\n",
            "   Pressure9am  Pressure3pm  Cloud9am  Cloud3pm  Temp9am  Temp3pm  RainToday  \\\n",
            "0       1007.7       1007.1       8.0       NaN     16.9     21.8         No   \n",
            "1       1010.6       1007.8       NaN       NaN     17.2     24.3         No   \n",
            "2       1007.6       1008.7       NaN       2.0     21.0     23.2         No   \n",
            "3       1017.6       1012.8       NaN       NaN     18.1     26.5         No   \n",
            "4       1010.8       1006.0       7.0       8.0     17.8     29.7         No   \n",
            "\n",
            "   RainTomorrow  \n",
            "0            No  \n",
            "1            No  \n",
            "2            No  \n",
            "3            No  \n",
            "4            No  \n",
            "\n",
            "[5 rows x 23 columns]\n",
            "\n",
            "üßæ DataFrame Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 145460 entries, 0 to 145459\n",
            "Data columns (total 23 columns):\n",
            " #   Column         Non-Null Count   Dtype  \n",
            "---  ------         --------------   -----  \n",
            " 0   Date           145460 non-null  object \n",
            " 1   Location       145460 non-null  object \n",
            " 2   MinTemp        143975 non-null  float64\n",
            " 3   MaxTemp        144199 non-null  float64\n",
            " 4   Rainfall       142199 non-null  float64\n",
            " 5   Evaporation    82670 non-null   float64\n",
            " 6   Sunshine       75625 non-null   float64\n",
            " 7   WindGustDir    135134 non-null  object \n",
            " 8   WindGustSpeed  135197 non-null  float64\n",
            " 9   WindDir9am     134894 non-null  object \n",
            " 10  WindDir3pm     141232 non-null  object \n",
            " 11  WindSpeed9am   143693 non-null  float64\n",
            " 12  WindSpeed3pm   142398 non-null  float64\n",
            " 13  Humidity9am    142806 non-null  float64\n",
            " 14  Humidity3pm    140953 non-null  float64\n",
            " 15  Pressure9am    130395 non-null  float64\n",
            " 16  Pressure3pm    130432 non-null  float64\n",
            " 17  Cloud9am       89572 non-null   float64\n",
            " 18  Cloud3pm       86102 non-null   float64\n",
            " 19  Temp9am        143693 non-null  float64\n",
            " 20  Temp3pm        141851 non-null  float64\n",
            " 21  RainToday      142199 non-null  object \n",
            " 22  RainTomorrow   142193 non-null  object \n",
            "dtypes: float64(16), object(7)\n",
            "memory usage: 25.5+ MB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import pandas as pd\n",
        "\n",
        "# Step 1: Unzip the file\n",
        "zip_path = '/content/weather.csv.zip'\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/')  # Extracts in the same directory\n",
        "\n",
        "# Step 2: Find the CSV file (assuming it's called 'weather.csv')\n",
        "# Corrected filename based on available files\n",
        "csv_path = '/content/weatherAUS.csv'\n",
        "\n",
        "# Step 3: Load the CSV file using pandas\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Step 4: Display some basic information\n",
        "print(\"‚úÖ File loaded successfully!\\n\")\n",
        "print(\"üìä First 5 rows of the dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\nüßæ DataFrame Info:\")\n",
        "print(df.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "546-7e1QJhdz",
        "outputId": "7c4152b1-4852-4e86-c388-61332fb77f51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üßæ Columns in dataset:\n",
            "Index(['Date', 'Location', 'MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation',\n",
            "       'Sunshine', 'WindGustDir', 'WindGustSpeed', 'WindDir9am', 'WindDir3pm',\n",
            "       'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm',\n",
            "       'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp9am',\n",
            "       'Temp3pm', 'RainToday', 'RainTomorrow'],\n",
            "      dtype='object')\n",
            "\n",
            "‚úÖ Features (X) shape: (145460, 22)\n",
            "üéØ Target (y) shape: (145460,)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load your data (adjust path if needed)\n",
        "df = pd.read_csv('/content/weatherAUS.csv')\n",
        "\n",
        "# OPTIONAL: Check the column names\n",
        "print(\"üßæ Columns in dataset:\")\n",
        "print(df.columns)\n",
        "\n",
        "# Define your target column\n",
        "target_column = 'RainTomorrow'  # Replace this with your actual target column name\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop(columns=[target_column])  # All columns except the target\n",
        "y = df[target_column]                 # Only the target column\n",
        "\n",
        "print(\"\\n‚úÖ Features (X) shape:\", X.shape)\n",
        "print(\"üéØ Target (y) shape:\", y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WgREyMf9KIe8",
        "outputId": "65ed17a9-d933-4135-9644-718d87b037f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Data split successfully!\n",
            "üèãÔ∏è Training data shape: (116368, 22)\n",
            "üß™ Validation data shape: (29092, 22)\n",
            "üéØ Training target shape: (116368,)\n",
            "üéØ Validation target shape: (29092,)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data into training and validation sets (you can adjust the test_size)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"‚úÖ Data split successfully!\")\n",
        "print(\"üèãÔ∏è Training data shape:\", X_train.shape)\n",
        "print(\"üß™ Validation data shape:\", X_val.shape)\n",
        "print(\"üéØ Training target shape:\", y_train.shape)\n",
        "print(\"üéØ Validation target shape:\", y_val.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gv1rQORbJ2_C"
      },
      "source": [
        "CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43jUNchvJ5Z9",
        "outputId": "6825fedf-fef0-4979-9782-e5b6fcf7654b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m3556/3556\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 7ms/step - accuracy: 0.8462 - loss: 0.3571 - val_accuracy: 0.8644 - val_loss: 0.3117\n",
            "Epoch 2/10\n",
            "\u001b[1m3556/3556\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 8ms/step - accuracy: 0.8804 - loss: 0.2786 - val_accuracy: 0.8685 - val_loss: 0.3016\n",
            "Epoch 3/10\n",
            "\u001b[1m3556/3556\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 8ms/step - accuracy: 0.8986 - loss: 0.2380 - val_accuracy: 0.8707 - val_loss: 0.3012\n",
            "Epoch 4/10\n",
            "\u001b[1m3556/3556\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 8ms/step - accuracy: 0.9148 - loss: 0.1989 - val_accuracy: 0.8700 - val_loss: 0.3238\n",
            "Epoch 5/10\n",
            "\u001b[1m3556/3556\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 7ms/step - accuracy: 0.9335 - loss: 0.1591 - val_accuracy: 0.8679 - val_loss: 0.3543\n",
            "Epoch 6/10\n",
            "\u001b[1m3556/3556\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 7ms/step - accuracy: 0.9486 - loss: 0.1258 - val_accuracy: 0.8642 - val_loss: 0.3899\n",
            "Epoch 7/10\n",
            "\u001b[1m3556/3556\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 7ms/step - accuracy: 0.9611 - loss: 0.0984 - val_accuracy: 0.8649 - val_loss: 0.4685\n",
            "Epoch 8/10\n",
            "\u001b[1m3556/3556\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 7ms/step - accuracy: 0.9727 - loss: 0.0733 - val_accuracy: 0.8611 - val_loss: 0.5383\n",
            "Epoch 9/10\n",
            "\u001b[1m3556/3556\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 7ms/step - accuracy: 0.9798 - loss: 0.0561 - val_accuracy: 0.8595 - val_loss: 0.5885\n",
            "Epoch 10/10\n",
            "\u001b[1m3556/3556\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 7ms/step - accuracy: 0.9852 - loss: 0.0409 - val_accuracy: 0.8591 - val_loss: 0.6996\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7d777af2b320>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Load the data (assuming the previous steps loaded it into df, X, and y)\n",
        "# If not, uncomment and run the following lines:\n",
        "# df = pd.read_csv('/content/weatherAUS.csv')\n",
        "# target_column = 'RainTomorrow'\n",
        "# X = df.drop(columns=[target_column])\n",
        "# y = df[target_column]\n",
        "\n",
        "# Split data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# --- Preprocessing Steps ---\n",
        "\n",
        "# Identify categorical and numerical features\n",
        "categorical_features = X_train.select_dtypes(include=['object']).columns\n",
        "numerical_features = X_train.select_dtypes(include=['float64', 'int64']).columns\n",
        "\n",
        "# Create preprocessing pipelines for numerical and categorical features\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')), # Impute missing numerical values\n",
        "    ('scaler', StandardScaler()) # Scale numerical features\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')), # Impute missing categorical values\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore')) # One-hot encode categorical features\n",
        "])\n",
        "\n",
        "# Combine preprocessing steps\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# Apply preprocessing to the training and validation data\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "X_val_processed = preprocessor.transform(X_val)\n",
        "\n",
        "# Preprocess target variable (RainTomorrow: Yes/No to 1/0)\n",
        "y_train_processed = y_train.apply(lambda x: 1 if x == 'Yes' else (0 if x == 'No' else -1)) # Map Yes to 1, No to 0, and others to -1\n",
        "y_val_processed = y_val.apply(lambda x: 1 if x == 'Yes' else (0 if x == 'No' else -1))\n",
        "\n",
        "# Remove rows where target variable is -1 (due to NaN in original y)\n",
        "valid_train_indices = y_train_processed != -1\n",
        "X_train_processed = X_train_processed[valid_train_indices.to_numpy()] # Convert boolean Series to numpy array\n",
        "y_train_processed = y_train_processed[valid_train_indices]\n",
        "\n",
        "valid_val_indices = y_val_processed != -1\n",
        "X_val_processed = X_val_processed[valid_val_indices.to_numpy()] # Convert boolean Series to numpy array\n",
        "y_val_processed = y_val_processed[valid_val_indices]\n",
        "\n",
        "\n",
        "# --- Model Definition and Training ---\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X_train_processed.shape[1],)),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')  # Use sigmoid for binary classification\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) # Use binary_crossentropy for binary classification\n",
        "model.fit(X_train_processed, y_train_processed, epochs=10, validation_data=(X_val_processed, y_val_processed)) # Reduced epochs for faster execution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7I8vrNtzKy0H"
      },
      "source": [
        "rnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-bXlVeeKyRg",
        "outputId": "3312ec02-92fa-4636-8ccd-81d9ab64bd97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m2804/3556\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m16:03\u001b[0m 1s/step - accuracy: 0.7764 - loss: 0.5027"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Input\n",
        "import numpy as np\n",
        "\n",
        "# Convert sparse matrices to dense NumPy arrays before reshaping\n",
        "X_train_processed_dense = X_train_processed.todense()\n",
        "X_val_processed_dense = X_val_processed.todense()\n",
        "\n",
        "# Reshape the input data for LSTM (assuming each feature is a timestep)\n",
        "# The shape should be (samples, timesteps, features)\n",
        "# Here, we treat each feature as a single timestep, so features = 1\n",
        "timesteps = X_train_processed_dense.shape[1]\n",
        "features = 1\n",
        "X_train_reshaped = np.reshape(X_train_processed_dense, (X_train_processed_dense.shape[0], timesteps, features))\n",
        "X_val_reshaped = np.reshape(X_val_processed_dense, (X_val_processed_dense.shape[0], timesteps, features))\n",
        "\n",
        "\n",
        "model = Sequential([\n",
        "    Input(shape=(timesteps, features)), # Explicitly define Input layer\n",
        "    LSTM(64, activation='tanh'),\n",
        "    Dense(1, activation='sigmoid') # Changed to sigmoid for binary classification\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) # Changed to binary_crossentropy for binary classification\n",
        "model.fit(X_train_reshaped, y_train_processed, epochs=10, validation_data=(X_val_reshaped, y_val_processed)) # Reduced epochs for faster execution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "encHpgifNMJX"
      },
      "source": [
        "shap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpcx2s0SNKfr"
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "import xgboost as xgb\n",
        "\n",
        "# Train an XGBoost model\n",
        "xgb_model = xgb.XGBRegressor()\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Explain the model using SHAP\n",
        "explainer = shap.Explainer(xgb_model)\n",
        "shap_values = explainer(X_test)\n",
        "\n",
        "# Plot global feature importance\n",
        "shap.summary_plot(shap_values, X_test)\n",
        "\n",
        "# Plot for one prediction\n",
        "shap.plots.waterfall(shap_values[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m20xtqJzNOc1"
      },
      "source": [
        "lime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gl6WcFbGNPw3"
      },
      "outputs": [],
      "source": [
        "import lime\n",
        "import lime.lime_tabular\n",
        "\n",
        "explainer = lime.lime_tabular.LimeTabularExplainer(\n",
        "    training_data=X_train.values,\n",
        "    feature_names=X.columns,\n",
        "    mode='regression'\n",
        ")\n",
        "\n",
        "# Explain one instance\n",
        "i = 5  # Index of the sample\n",
        "exp = explainer.explain_instance(X_test.iloc[i].values, xgb_model.predict)\n",
        "\n",
        "# Show explanation\n",
        "exp.show_in_notebook(show_table=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4N2pCXfJOc8-"
      },
      "source": [
        "pdp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LuWXohHkOcBx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.inspection import PartialDependenceDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv(\"/content/weatherAUS.csv\")\n",
        "\n",
        "# Drop rows with missing target\n",
        "data = data.dropna(subset=['RainTomorrow'])\n",
        "\n",
        "# Select features (you can adjust this list)\n",
        "features = ['MinTemp', 'MaxTemp', 'Rainfall', 'Humidity9am', 'Humidity3pm', 'WindSpeed9am']\n",
        "X = data[features]\n",
        "y = data['RainTomorrow']\n",
        "\n",
        "# Encode target\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "\n",
        "# Handle missing values\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X = pd.DataFrame(imputer.fit_transform(X), columns=features)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train RandomForest\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Partial Dependence Plot for selected features\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "PartialDependenceDisplay.from_estimator(model, X_test, features=['MaxTemp', 'Humidity3pm'], ax=ax)\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "128LSNHyirbHq7yC3MRHZv16khrEQdfbY",
      "authorship_tag": "ABX9TyPBRanMkf32RQ2sirI6AnMd",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}